% \VignetteIndexEntry{SuperLearner manual}
% \VignetteDepends{SuperLearner}
% \VignettePackage{SuperLearner}
\documentclass[11pt, nojss]{jss}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{longtable}

\DeclareMathOperator{\argmax}{\arg\max}
\DeclareMathOperator{\argmin}{\arg\min}

%% need no \usepackage{Sweave.sty}
\SweaveOpts{keep.source=TRUE}
\SweaveOpts{eps = FALSE, pdf = TRUE}  % only produce pdf
\SweaveOpts{width=6, height=4}  % default figure size in R
\setkeys{Gin}{width=0.85\textwidth}   % default figure size in latex
<<setup, echo=false, results = hide>>=
options(width=60)
if(!file.exists('plots')) dir.create('plots')
# options(continue=' ')
@
\SweaveOpts{prefix.string = plots/fig}

\title{\texttt SuperLearner}
\author{Eric C. Polley\\ Biometric Research Branch\\ National Cancer Institute \And Mark J. van der Laan\\ Division of Biostatistics\\ University of California, Berkeley }
\Plainauthor{Eric C. Polley, Mark J. van der Laan}
\Plaintitle{The SuperLearner Package}
\Address{
  Eric Polley\\
  Biometric Research Branch\\
  National Cancer Institute\\
  E-mail: \email{eric.polley@nih.gov}\\
  URL: \url{http://linus.nci.nih.gov/}\\
  
	Mark van der Laan\\
	Division of Biostatistics\\
	University of California, Berkeley\\
	E-mail: \email{laan@berkeley.edu}\\
	URL: \url{http://www.stat.berkeley.edu/~laan/}
}
\Abstract{
	An \proglang{R} package for the Super Learner \cite{superlearner:2007} is presented.
}
\Keywords{prediction models, cross-validation, \proglang{R}}
\Plainkeywords{prediction models, cross-validation, R}

\begin{document}

\maketitle

\section{Introduction}
This vignette explains how to use the \pkg{SuperLearner} \proglang{R} package. The \pkg{SuperLearner} package provides a syntax and structure to implement the super learner algorithm \cite{superlearner:2007}.  A characteristic of the super learner algorithm is the ability to combine many different prediction algorithms together and let the data decide on the optimal ensemble.  \proglang{R} is the perfect language for such an algorithm because of the wealth of available prediction algorithms already available in the community.  One problem is that prediction algorithms do not have a common syntax, so one of the goals of the \pkg{SuperLearner} package is to translate these prediction algorithms into a common syntax to allow for easy programming of the super learner.

\section{The Super Learner Algorithm}

% \begin{table}[!htbp]
%    \centering
   % \topcaption{Table captions are better up top} % requires the topcapt package
   \begin{longtable}{llll @{} } % @{} suppresses leading/trailing space
	\caption{Details of prediction algorithm wrappers \label{tab:predAlg}} \\
	   \toprule
		\textbf{Function} & \textbf{Package} & \textbf{Tuning Parameters} & \textbf{Description} \tabularnewline
		\midrule
	\endhead
	
	\midrule
	\multicolumn{4}{c}{(Continued on next page)}
	\endfoot
	
	\bottomrule
	\endlastfoot
	
		\code{bagging} & \pkg{ipred} & \code{nbagg} & Bagging CART trees \\*
					&				& \code{minsplit} \\*
					&				& \code{cp} \\*
					&				& \code{maxdepth}\tabularnewline
		\code{bart}	& \pkg{BayesTree} & \code{ntree} & Bayesian Regression Trees \\*
					&				& \code{sigdf} \\*
					&				& \code{sigquant}\\*
					&				& \code{k}\\*
					&				& \code{power}\\*
					&				& \code{base}\\*
					&				& \code{ndpost}\\*
					&				& \code{nskip}\tabularnewline
	\code{bayesglm} & \pkg{arm}	& \code{prior.mean} & Bayesian glm \\*
					&				& \code{prior.scale}\\*
					&				& \code{prior.df}\tabularnewline
	\code{cforest}	& \pkg{party}	& \code{ntree} & Conditional Tree Forest\\*
	 				& 				& \code{mtry}\\*
					&				& \code{mincriterion}\\*
					&				& \code{teststat}\\*
					&				& \code{testtype}\\*
					&				& \code{replace}\\*
					&				& \code{fraction}\tabularnewline
	\code{cv.spls} 	& \pkg{spls}	& \code{K} & Sparse partial least squares \\*
					&				& \code{eta}\tabularnewline
	\code{DSA} 	& \pkg{DSA} 	& \code{maxsize} & Deletion\textbackslash Substitution\textbackslash Addition\\*
					&				& \code{maxorderint} \\*
					&				& \code{maxsumofpow} \\*
					&				& \code{Dmove} \\*
					&				& \code{Smove} \\*
					&				& \code{vfold} \tabularnewline
	\code{earth}	& \pkg{earth}	& \code{degree} & Adaptive Regression Splines \\*
					&				& \code{penalty} \\*
					&				& \code{nk} \\*
					&				& \code{thresh} \\*
					&				& \code{minspan} \\*
					&				& \code{newvar.penalty} \\*
					&				& \code{fast.k} \\*
					&				& \code{fast.beta} \\*
					&				& \code{nfold} \\*
					&				& \code{pmethod} \tabularnewline
	\code{gam}		& \pkg{gam}		& \code{deg.gam} & Generalized additive model \tabularnewline
	\code{gbm}		& \pkg{gbm}		& \code{gbm.trees} & Gradient boosting \\*
					&				& \code{interaction.depth} \\*
					&				& \code{cv.folds} \\*
					&				& \code{shrinkage} \\*
					&				& \code{n.minobsinnode} \\*
					&				& \code{bag.fraction} \\*
					&				& \code{train.fraction}\tabularnewline
	\code{glm}		& \pkg{stats}	& -- & Generalized linear model \tabularnewline
	\code{glmnet}	& \pkg{glmnet}	& \code{alpha} & Elastic Net \\*
					&				& \code{lambda}\\*
					&				& \code{nlambda}\\*
					&				& \code{lambda.min}\\*
					&				& \code{dfmax}\\*
					&				& \code{type}\tabularnewline
	\code{knn}		& \pkg{class}	& \code{k} & k-Nearest neighbors\\*
					&				& \code{use.all}\tabularnewline
	\code{loess}	& \pkg{stats}	& \code{span} & Local polynomial regression\\*
					&				& \code{family}\\*
					&				& \code{degree}\tabularnewline
	\code{logreg}	& \pkg{LogicReg} & \code{ntrees} & Logic regression\\*
					&				& \code{nleaves}\\*
					&				& \code{select}\\*
					&				& \code{penalty}\\*
					&				& \code{kfold}\\*
					&				& \code{control}\tabularnewline
	\code{mars}		& \pkg{mda}		& \code{degree} & Adaptive Regression Splines \\*
					&				& \code{nk} \\*
					&				& \code{penalty} \\*
					&				& \code{thresh} \\*
					&				& \code{prune} \\*
					&				& \code{forward.step} \tabularnewline
	\code{nnet}		& \pkg{nnet}	& \code{size} & Neural network\\*
					&				& \code{decay}\\*
					&				& \code{rang}\tabularnewline
	\code{polymars}	& \pkg{polspline} & \code{maxsize} & Adaptive polynomial splines \\*
					&				& \code{gcv}\\*
					&				& \code{additive}\\*
					&				& \code{knots}\\*
					&				& \code{know.space} \tabularnewline
	\code{polyclass} & \pkg{polspline} & \code{maxdim} & Polychotomous regression \\*
					&				& \code{cv}\\*
					&				& \code{additive}\tabularnewline
	\code{randomForest} & \pkg{randomForest} & \code{ntree} & Random Forest \\*
					&				& \code{mtry}\\*
					&				& \code{nodesizes}\\*
					&				& \code{sampsize}\\*
					&				& \code{replace}\\*
					&				& \code{maxnodes}\tabularnewline
	\code{Ridge}	& \pkg{MASS}	& \code{lambda} & Ridge regression\tabularnewline
	\code{rpart}	& \pkg{rpart}	& \code{cp} & Regression tree\\*
					&				& \code{minsplit}\\*
					&				& \code{xval}\\*
					&				& \code{maxdepth}\\*
					&				& \code{minbucket}\tabularnewline
	\code{step}		& \pkg{stats}	& \code{scope} & Stepwise regression\\*
	 				&				& \code{scale}\\*
					&				& \code{direction}\\*
					&				& \code{steps}\\*
					&				& \code{k}\tabularnewline
	\code{step.plr} & \pkg{stepPlr} & \code{type} & Stepwise penalized logistic\\*
					&				& \code{lambda}\\*
					&				& \code{cp}\\*
					&				& \code{max.terms}\tabularnewline
	\code{svm}		& \pkg{e1071}	& \code{type} & Support vector machine\\*
					&				& \code{kernel}\\*
					&				& \code{nu}\\*
					&				& \code{degree}\\*
					&				& \code{gamma}\\*
					&				& \code{coef0}\\*
					&				& \code{cost}\\*
					&				& \code{cachesize}\\*
					&				& \code{tolerance}\\*
					&				& \code{epsilon}\\*
					&				& \code{cross}\tabularnewline
      % \bottomrule
   \end{longtable}
   % \caption{Details of prediction algorithm wrappers}
%    \label{tab:predAlg}
% \end{table}

\section{Using the SuperLearner Package}
<<>>=
library(SuperLearner)

@
\subsection{Creating prediction wrappers}
A full list of the built-in prediction wrappers can be found with the function:
<<>>=
listWrappers(what = 'SL')
@

\subsection{Creating screening wrappers}
A full list of the built-in screening wrappers can be found with the function:
<<>>=
listWrappers(what = 'screen')
@

\subsection{Creating methods wrappers}

\section{Examples}
Below we work out a few examples using the \pkg{SuperLearner} package with real data.

\subsection{Boston Housing example}
The Boston housing data available in the \pkg{mlbench} package contains information on the housing prices from 506 census tracts around Boston from the 1970 census. The original data description can be found in \cite{Harrison:1978} and a correction in \cite{Gilley:1996}. Here we build an estimator to predict the median value of owner-occupied homes based on the covariates supplied. First we load the data and examine the variables includes:
<<>>=
library(mlbench)
data(BostonHousing2)
@ 
With the data loaded, we need to change the \code{chas} variable from a factor to a numeric and then remove the variables we will not be using in the analysis.
<<>>=
BostonHousing2$chas <- as.numeric(BostonHousing2$chas=="1")
DATA <- BostonHousing2[, c("cmedv", "crim", "zn", "indus", "chas", "nox",
  "rm", "age", "dis", "rad", "tax", "ptratio", "b", "lstat")]
@

<<echo=FALSE, eval=TRUE, results=hide>>=
library(ggplot2)
@

\begin{figure}[htbp]
   \centering
<<fig=TRUE, include=TRUE, eval=TRUE, echo=FALSE>>=
print(qplot(cmedv, data=DATA, binwidth=2, xlab = "Median home values", ylab = "Count"))
@
   \caption{Histogram of median home values (in 1000 USD)}
   \label{fig:histY}
\end{figure}

Before estimating the super learner, the library of prediction algorithms needs to be put together. As demonstrated above, the package contains a wide variety of built in algorithms. But the available algorithms may not be sufficient for any given problem.

For example, the \code{SL.gam} function has the tuning parameter \code{deg.gam = 2}.  If we wanted to also consider the \code{gam} model with degree 3 we do not need to write a new wrapper function but can easily use the current function and change the argument.  For example:
<<>>=
SL.gam.3 <- function(..., deg.gam = 3){
	SL.gam(..., deg.gam = deg.gam)
}
@
\code{SL.gam.3} is a simple function calling the \code{SL.gam} function but changing the argument \code{deg.gam} to the desired value for the tuning parameter.  The ``\ldots'' will simply pass all other arguments from \code{SL.gam.3} into \code{SL.gam} since these should be the same.  This mechanism of writing functions of the wrapper functions allows for the easy adaptation of the currently available prediction algorithms to the current prediction problem and the desired range for tuning parameters.  We will also add a few more values for \code{deg.gam}:
<<>>=
SL.gam.4 <- function(..., deg.gam = 4){
	SL.gam(..., deg.gam = deg.gam)
}
SL.gam.5 <- function(..., deg.gam = 5){
	SL.gam(..., deg.gam = deg.gam)
}
@
Many of the built-in wrappers have tuning parameters, and for the super learner we may want to try a range of values for this tuning parameters. Instead of creating the functions one-by-one as in the \code{SL.gam} example above, we could write a loop.  For example, the function \code{glmnet} has a tuning parameter \code{alpha} controlling the mixture of the penalty terms in the elastic net regression. Below we have a function to loop over the different values of the tuning parameter and write a new function in the global environment with the appropriate value for \code{alpha}:
<<>>=
create.SL.glmnet <- function(alpha = c(0.25, 0.50, 0.75)) {
  for(mm in seq(length(alpha))){
    eval(parse(text = paste('SL.glmnet.', alpha[mm], '<- function(..., alpha = ', alpha[mm], ') SL.glmnet(..., alpha = alpha)', sep = '')), envir = .GlobalEnv)
  }
  invisible(TRUE)
}
create.SL.glmnet(alpha = c(0.2, 0.4, 0.6, 0.8, 1.0))
@



\bibliography{SLrefs}

\section{Computing Environment}
% These analyses were done using the following versions of R\cite{Rsystem}, the
% operating system, and add-on packages \code{Hmisc}\cite{Hmisc},
% \code{Design}\cite{Design}, and others:
<<echo=F, results=tex>>=
s <- toLatex(sessionInfo())
cat(s[-grep('Locale',s)], sep='\n')
@

\end{document}
